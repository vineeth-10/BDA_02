{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHEchvk9t7i1ZMT6N8HvU+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vineeth-10/BDA_02/blob/main/BDA_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Build a Classification Model with Spark with a dataset of your choice**"
      ],
      "metadata": {
        "id": "IAdygPHY6h_y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36LVnCYizEBL",
        "outputId": "dc2a6678-a0c4-4f7a-95ce-1e70465f8ff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy = 1.00\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Step 1: Start Spark Session\n",
        "spark = SparkSession.builder.appName(\"IrisClassification\").getOrCreate()\n",
        "\n",
        "# Step 2: Load Dataset\n",
        "df = spark.read.csv(\"/content/iris.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Step 3: Convert label (species) to numeric\n",
        "indexer = StringIndexer(inputCol=\"species\", outputCol=\"label\")\n",
        "df = indexer.fit(df).transform(df)\n",
        "\n",
        "# Step 4: Assemble features into a vector\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "data = assembler.transform(df).select(\"features\", \"label\")\n",
        "\n",
        "# Step 5: Split dataset into training and test sets\n",
        "train_data, test_data = data.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# Step 6: Train Logistic Regression Model\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "model = lr.fit(train_data)\n",
        "\n",
        "# Step 7: Make Predictions\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Step 8: Evaluate Model Accuracy\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"Test Accuracy = {accuracy:.2f}\")\n",
        "\n",
        "# Stop Spark Session\n",
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Build a Clustering Model with Spark with a dataset of your choice**"
      ],
      "metadata": {
        "id": "YaicmW7Y60tR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "# Step 1: Create Spark session\n",
        "spark = SparkSession.builder.appName(\"IrisClustering\").getOrCreate()\n",
        "\n",
        "# Step 2: Load dataset\n",
        "df = spark.read.csv(\"/content/iris.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Step 3: Drop the label column (species) for unsupervised learning\n",
        "df = df.drop(\"species\")\n",
        "df = df.toDF(*[c.strip() for c in df.columns])  # Remove any extra spaces\n",
        "\n",
        "# Step 4: Assemble features\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "feature_data = assembler.transform(df).select(\"features\")\n",
        "\n",
        "# Step 5: Train KMeans model\n",
        "kmeans = KMeans(k=3, seed=1)  # 3 clusters (one for each Iris species)\n",
        "model = kmeans.fit(feature_data)\n",
        "\n",
        "# Step 6: Make predictions\n",
        "predictions = model.transform(feature_data)\n",
        "\n",
        "# Step 7: Evaluate clustering using Silhouette score\n",
        "evaluator = ClusteringEvaluator()\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"Silhouette Score = {silhouette:.2f}\")\n",
        "print(\"Cluster Centers:\")\n",
        "for center in model.clusterCenters():\n",
        "    print(center)\n",
        "\n",
        "# Stop Spark\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8Tw7Wt-5oZP",
        "outputId": "ff6fc645-7c48-4916-f548-ce090f186933"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Score = 0.59\n",
            "Cluster Centers:\n",
            "[4.9        3.23333333 1.36666667 0.2       ]\n",
            "[6.  2.2 5.  1.5]\n",
            "[6.4 2.8 5.6 2.1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Build a Recommendation Engine with Spark with a dataset of your\n",
        "choice**\n"
      ],
      "metadata": {
        "id": "IdWV_tqn7QFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Step 1: Start Spark Session\n",
        "spark = SparkSession.builder.appName(\"MovieRecommendation\").getOrCreate()\n",
        "\n",
        "# Step 2: Load or create a ratings dataset\n",
        "# Sample data: userId, movieId, rating\n",
        "data = [\n",
        "    (0, 0, 4.0), (0, 1, 2.0), (0, 2, 5.0),\n",
        "    (1, 0, 5.0), (1, 2, 3.0),\n",
        "    (2, 1, 4.0), (2, 2, 2.0)\n",
        "]\n",
        "columns = [\"userId\", \"movieId\", \"rating\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Step 3: Split the data\n",
        "(training, test) = df.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Step 4: Build ALS recommendation model\n",
        "als = ALS(\n",
        "    maxIter=10,\n",
        "    regParam=0.1,\n",
        "    userCol=\"userId\",\n",
        "    itemCol=\"movieId\",\n",
        "    ratingCol=\"rating\",\n",
        "    coldStartStrategy=\"drop\"\n",
        ")\n",
        "model = als.fit(training)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "predictions = model.transform(test)\n",
        "evaluator = RegressionEvaluator(\n",
        "    metricName=\"rmse\",\n",
        "    labelCol=\"rating\",\n",
        "    predictionCol=\"prediction\"\n",
        ")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f\"Root-mean-square error = {rmse:.2f}\")\n",
        "\n",
        "# Step 6: Generate top 3 movie recommendations for each user\n",
        "user_recs = model.recommendForAllUsers(3)\n",
        "user_recs.show(truncate=False)\n",
        "\n",
        "# Step 7: Generate top 3 user recommendations for each movie\n",
        "movie_recs = model.recommendForAllItems(3)\n",
        "movie_recs.show(truncate=False)\n",
        "\n",
        "# Stop Spark\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGKARCVF6BSz",
        "outputId": "608c7417-e490-4b25-8801-cef7c6078e2d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root-mean-square error = 3.05\n",
            "+------+------------------------------------------------+\n",
            "|userId|recommendations                                 |\n",
            "+------+------------------------------------------------+\n",
            "|0     |[{0, 3.9031894}, {1, 2.0078704}, {2, 1.0150895}]|\n",
            "|1     |[{0, 4.941399}, {1, 2.6577547}, {2, 1.3459307}] |\n",
            "|2     |[{1, 3.8469234}, {0, 3.6589017}, {2, 1.983662}] |\n",
            "+------+------------------------------------------------+\n",
            "\n",
            "+-------+------------------------------------------------+\n",
            "|movieId|recommendations                                 |\n",
            "+-------+------------------------------------------------+\n",
            "|0      |[{1, 4.941399}, {0, 3.9031894}, {2, 3.6589017}] |\n",
            "|1      |[{2, 3.8469234}, {1, 2.6577547}, {0, 2.0078704}]|\n",
            "|2      |[{2, 1.983662}, {1, 1.3459307}, {0, 1.0150895}] |\n",
            "+-------+------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}